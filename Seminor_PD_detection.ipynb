{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_lfzxRnLxkn",
        "outputId": "e4052f87-d999-402c-8b4e-86d03498fb27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ntua-parkinson-dataset'...\n",
            "remote: Enumerating objects: 42113, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 42113 (delta 0), reused 4 (delta 0), pack-reused 42104 (from 1)\u001b[K\n",
            "Receiving objects: 100% (42113/42113), 2.14 GiB | 32.95 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Updating files: 100% (44019/44019), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ails-lab/ntua-parkinson-dataset.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the .git folder to avoid ImageFolder errors\n",
        "!rm -rf /content/ntua-parkinson-dataset/.git\n",
        "\n",
        "# Now you can safely list and load your dataset folder\n",
        "!ls /content/ntua-parkinson-dataset\n",
        "\n",
        "# Remove all .ipynb_checkpoints folders recursively\n",
        "!find /content/ntua-parkinson-dataset -type d -name \".ipynb_checkpoints\" -exec rm -rf {} +\n",
        "\n",
        "# Then continue with your dataset loading code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwJ5LE0pOO07",
        "outputId": "8655ccb2-c437-49fa-939e-452fc930ecde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Non PD Patients'  'PD Patients'   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Dataset root folder (contains PD and NPD subfolders with images)\n",
        "base_dir = \"/content/ntua-parkinson-dataset\"\n",
        "\n",
        "# Data transforms matching VGG19 input requirements\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset (ImageFolder reads classes from folder names)\n",
        "dataset = datasets.ImageFolder(base_dir, transform=transform)\n",
        "print(\"Classes found:\", dataset.classes)\n",
        "\n",
        "# Create train, val, test splits (stratified)\n",
        "indices = list(range(len(dataset)))\n",
        "train_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=dataset.targets, random_state=42)\n",
        "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=[dataset.targets[i] for i in test_idx], random_state=42)\n",
        "\n",
        "train_ds = Subset(dataset, train_idx)\n",
        "val_ds = Subset(dataset, val_idx)\n",
        "test_ds = Subset(dataset, test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32)\n",
        "test_loader = DataLoader(test_ds, batch_size=32)\n",
        "\n",
        "# Load pretrained VGG19 model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "# Replace the last classifier layer for 2 classes\n",
        "model.classifier[6] = nn.Linear(4096, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, correct = 0, 0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "    return running_loss / len(loader.dataset), correct / len(loader.dataset)\n",
        "\n",
        "# Validation function\n",
        "def eval_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    return running_loss / len(loader.dataset), correct / len(loader.dataset)\n",
        "\n",
        "# Train loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Test evaluation and classification report\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = outputs.argmax(dim=1).cpu()\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKKQ-AteM0F9",
        "outputId": "121a9eaa-c676-4e3a-a2b1-d231a78f16da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: ['Non PD Patients', 'PD Patients']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:05<00:00, 99.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 — Train Loss: 0.3784, Train Acc: 0.8259, Val Loss: 0.2235, Val Acc: 0.9048\n",
            "Epoch 2/5 — Train Loss: 0.1816, Train Acc: 0.9223, Val Loss: 0.1789, Val Acc: 0.9237\n",
            "Epoch 3/5 — Train Loss: 0.1153, Train Acc: 0.9539, Val Loss: 0.1159, Val Acc: 0.9566\n",
            "Epoch 4/5 — Train Loss: 0.0817, Train Acc: 0.9687, Val Loss: 0.0790, Val Acc: 0.9727\n",
            "Epoch 5/5 — Train Loss: 0.0564, Train Acc: 0.9791, Val Loss: 0.0683, Val Acc: 0.9755\n",
            "\n",
            "Test Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Non PD Patients       0.96      0.96      0.96      1071\n",
            "    PD Patients       0.99      0.99      0.99      3330\n",
            "\n",
            "       accuracy                           0.98      4401\n",
            "      macro avg       0.97      0.97      0.97      4401\n",
            "   weighted avg       0.98      0.98      0.98      4401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set the same base directory used before\n",
        "base_dir = \"/content/ntua-parkinson-dataset\"  # Change if needed\n",
        "\n",
        "# Transform for ResNet input (same as VGG)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = datasets.ImageFolder(base_dir, transform=transform)\n",
        "\n",
        "# Stratified split\n",
        "indices = list(range(len(dataset)))\n",
        "train_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=dataset.targets, random_state=42)\n",
        "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=[dataset.targets[i] for i in test_idx], random_state=42)\n",
        "\n",
        "train_ds = Subset(dataset, train_idx)\n",
        "val_ds = Subset(dataset, val_idx)\n",
        "test_ds = Subset(dataset, test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32)\n",
        "test_loader = DataLoader(test_ds, batch_size=32)\n",
        "\n",
        "# Load pretrained ResNet50\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Replace the final layer for 2 classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, correct = 0, 0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "    return running_loss / len(loader.dataset), correct / len(loader.dataset)\n",
        "\n",
        "# Evaluation function\n",
        "def eval_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    return running_loss / len(loader.dataset), correct / len(loader.dataset)\n",
        "\n",
        "# Train ResNet50\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Final test evaluation\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = outputs.argmax(dim=1).cpu()\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90WJuKf1g8bW",
        "outputId": "f2419114-5535-4373-bd3e-5b4c87864198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 164MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 — Train Loss: 0.2433, Acc: 0.8937, Val Loss: 0.1431, Acc: 0.9446\n",
            "Epoch 2/5 — Train Loss: 0.0985, Acc: 0.9601, Val Loss: 0.1017, Acc: 0.9616\n",
            "Epoch 3/5 — Train Loss: 0.0638, Acc: 0.9753, Val Loss: 0.0603, Acc: 0.9773\n",
            "Epoch 4/5 — Train Loss: 0.0427, Acc: 0.9846, Val Loss: 0.0682, Acc: 0.9736\n",
            "Epoch 5/5 — Train Loss: 0.0317, Acc: 0.9882, Val Loss: 0.0498, Acc: 0.9818\n",
            "\n",
            "Test Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Non PD Patients       0.95      0.97      0.96      1071\n",
            "    PD Patients       0.99      0.98      0.99      3330\n",
            "\n",
            "       accuracy                           0.98      4401\n",
            "      macro avg       0.97      0.98      0.97      4401\n",
            "   weighted avg       0.98      0.98      0.98      4401\n",
            "\n"
          ]
        }
      ]
    }
  ]
}